text_osfi = """Introduction
This Guideline outlines OSFI’s expectations for the establishment of an enterprise-wide model
risk management framework at institutions. Taking an enterprise-wide view of risk implies that
these principles should be appropriately applied across the entire spectrum of models used by
institutions for risk management purposes. It is an institution’s responsibility to develop a
consistent set of policies and/or procedures in order to identify, assess, manage and control the
risks inherent in any model.
OSFI recognizes that large complex institutions with internal models approved for regulatory
capital purposes already have the necessary risk control infrastructure in place, whereas others
might apply controls only in materially relevant areas. As a result, this Guideline should be
interpreted in the context of a proportionality principle whereby applicability is commensurate
with the nature, size, complexity, and risk profile of the institution.
OSFI will distinguish between internal models approved institutions (IMAIs) and other
standardized institutions (SIs). The dividing line will be contingent on whether or not the
institution has received OSFI approval to use an internal model for regulatory capital purposes. If
approval has been granted then the institution is considered to be among the IMAI cohort;
otherwise it will be treated as an SI.
• IMAIs should comply with all components of this Guideline.
• SIs should strive to comply with this Guideline.
Definitions
Model – A model generally refers to a methodology, system, and/or approach that applies
theoretical and (expert) judgmental assumptions and statistical techniques to process input data
in order to generate quantitative estimates. A model has three distinct components: i) a data input
component that may also include relevant assumptions; ii) a processing component that translates
the inputs into estimates; and iii) a result component that presents these estimates in a format that
is useful and meaningful to business lines and control functions.
Model risk – The risk of adverse financial (e.g., capital, losses, revenue) and reputational
consequences arising from the design, development, implementation and/or use of a model. It
can originate from, among other things, inappropriate specification; incorrect parameter
estimates; flawed hypotheses and/or assumptions; mathematical computation errors; inaccurate,
inappropriate or incomplete data; inappropriate, improper or unintended usage; and inadequate
monitoring and/or controls.
Model user – The unit(s)/individual(s) that relies on the model’s outputs as a basis for making
business decisions. While model users may be involved in the early stages of model development
and ongoing monitoring activities, this involvement is no substitution for independent and
objective review.
Model developer – The unit(s)/individual(s) responsible for designing, developing, evaluating
and documenting models which may also perform ongoing monitoring and outcomes analysis as
well as periodic reassessment once a model is in use.
Model owner – The unit(s)/individual(s) responsible for the model selection, coordinating model
development, initial testing, ongoing monitoring, outcomes analysis, administering changes and
documentation. The model owner could also be the model developer or user.
Model reviewer – The independent unit(s) responsible for model vetting, validation and
reporting its findings and recommendations to the model approver. Other responsibilities might
include providing the model developer and user with guidance on the appropriateness of models
for defined purpose.
Model approver – The individual(s) and/or committee(s) responsible for assessing the model
reviewer’s findings and recommendations and approving the use and/or limitation of use of any
new model or changes to pre-existing models. Depending on the size and complexity of the institution, along with the materiality of the model being reviewed, it may be acceptable for the
roles of model reviewer and approver to be combined as long as there is no potential conflict of
interest and independence is maintained. For the purposes of this guideline, the terms ‘model
risk committee’ and ‘model approver’ are used interchangeably."""

text_eu = '''The purpose of this Regulation is to improve the functioning of the internal market by
laying down a uniform legal framework in particular for the development, the placing on
the market, the putting into service and the use of artificial intelligence systems (AI
systems) in the Union, in accordance with Union values, to promote the uptake of human
centric and trustworthy artificial intelligence (AI) while ensuring a high level of
protection of health, safety, fundamental rights as enshrined in the Charter of
Fundamental Rights of the European Union (the ‘Charter’) including democracy, the
rule of law and environmental protection, to protect against the harmful effects of AI
systems in the Union, and to support innovation. 
This Regulation ensures the free
movement, cross-border, of AI-based goods and services, thus preventing Member States
from imposing restrictions on the development, marketing and use of AI systems, unless
explicitly authorised by this Regulation.
This Regulation should be applied in accordance with the values of the Union enshrined
as in the Charter, facilitating the protection of natural persons, undertakings,
democracy, the rule of law and environmental protection, while boosting innovation and
employment and making the Union a leader in the uptake of trustworthy AI.
▌ AI systems ▌ can be easily deployed in a large variety of sectors of the economy and
many parts of society, including across borders, and can easily circulate throughout the
Union. Certain Member States have already explored the adoption of national rules to
ensure that AI is trustworthy and safe and is developed and used in accordance with
fundamental rights obligations. 
Diverging national rules may lead to the fragmentation of
the internal market and may decrease legal certainty for operators that develop, import or
use AI systems. 
A consistent and high level of protection throughout the Union should
therefore be ensured in order to achieve trustworthy AI, while divergences hampering the
free circulation, innovation, deployment and the uptake of AI systems and related
products and services within the internal market should be prevented by laying down
uniform obligations for operators and guaranteeing the uniform protection of overriding
reasons of public interest and of rights of persons throughout the internal market on the
basis of Article 114 of the Treaty on the Functioning of the European Union (TFEU). 
To the extent that TFEU contains specific rules on the protection of individuals with
regard to the processing of personal data concerning restrictions of the use of AI systems
for remote biometric identification for the purpose of law enforcement, of the use of AI
systems for risk assessments of natural persons for the purpose of law enforcement and
of the use of AI systems of biometric categorisation for the purpose of law enforcement, it
is appropriate to base this Regulation, in so far as those specific rules are concerned, on
Article 16 TFEU. In light of those specific rules and the recourse to Article 16 TFEU, it is
appropriate to consult the European Data Protection Board.
AI is a fast evolving family of technologies that contributes to a wide array of economic,
environmental and societal benefits across the entire spectrum of industries and social
activities. 
By improving prediction, optimising operations and resource allocation, and
personalising digital solutions available for individuals and organisations, the use of AI can
provide key competitive advantages to undertakings and support socially and
environmentally beneficial outcomes.
Environmentally beneficial outcomes include healthcare, agriculture, food safety,
education and training, media, sports, culture, infrastructure management, energy,
transport and logistics, public services, security, justice, resource and energy efficiency,
environmental monitoring, the conservation and restoration of biodiversity and
ecosystems and climate change mitigation and adaptation.
At the same time, depending on the circumstances regarding its specific application, use,
and level of technological development, AI may generate risks and cause harm to public
interests and fundamental rights that are protected by Union law. Such harm might be
material or immaterial, including physical, psychological, societal or economic harm.'''
